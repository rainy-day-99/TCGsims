# TCG Sims
This repository is a colleciton of tools for simulating trading card games, in particular the game Cardfight!! Vanguard.
## What am I looking at?
In `gametools.py` there are three primary classes:
+ `Card` represents a single card type. The `min` and `max` attributes put limits on how many copies of that card can exist in  a deck. They can be hashed and well-ordered.
+ `Decklist` holds a dictionary and numpy array. The dictionary keys are `Card` objects, and its value is the number of cards in a deck. The array should hold the results of one simulated game with the deck played.
+ `GameEnvironment` holds the relevant information for one particular test. It has a method for creating an initial `Decklist` to start a search algorithm, one for actually running a given number of games, and one for "scoring" a decklist.

The testing itself is done in `main.py`, which uses different search procedures found in `montecarlo.py` and `bruteforce.py`. It imports a `GameEnvironment` from one of the test modules found in the folders, runs a search procedure, then outputs a table of the decks tested and their scores.
## What are the test procedures?
The two primary ones are `montecarlo.py` and `bruteforce.py`. 

The first is a *local search* algorithm, mainly inspired by Frank Karsten's search algorithm. 
+ A starting deck is generated using the `GameEnvironment`, then "nearby" decks are generated by adding one card and removing another. Limits on what the lowest and highest a card can go are defined in its `max` and `min` attribute. 
+ A small number of simulations are run with each deck in the neighborhood of that seed deck
+ The best deck in the neighborhood is selected to be the center of the next neighborhood. The minimum number of simulations is increased slightly.
+ To save time, decks are skipped because they either hit the simulation limit, or we're confident they won't have a better score than the previous best deck. This is determined using a Student's $t$-test ($\alpha$=0.01).
+ The search is terminated when no decks beside the best local deck are tested.

The second is a *brute-force* search of every possible deck. This iteratively runs games with every deck, sorts every deck according to its score, and skips testing using the simulation limit or Student's $t$-test. This repeats again until no decks are tested. 

The brute-force search is best used when you're interested in the scores and means of every deck and they can all be tested in a reasonable amount of time. The local search is best when this is not feasible.

## How are these used?
The results of the searches are meant to help guide deckbuilding decisions, but should not be taken as gospel. Several assumptions are made in the creation of individual sims, and I cannot claim that any results are "optimal" as a result. However, it is helpful to have some numbers to refer to in conversations surrounding deckbuilding, and these simulations should help provide that.